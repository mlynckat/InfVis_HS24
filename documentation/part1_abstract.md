# Visualizing Performance Patterns of Open-Source Large Language Models

## Abstract

We want to analyze and visualize data from the [Open LLM Leaderboard Dataset](https://huggingface.co/datasets/open-llm-leaderboard/contents), a compilation of performance metrics for a wide range of open-source large language models. The dataset, available at Hugging Face, contains 1,420 rows and 34 attributes, including precision scores, model architectures, parameter counts, and evaluation metrics across various tasks such as IFEval, GPQA, and MATH.

Our primary goal is to explore the data and uncover patterns in LLM performance across different tasks, model sizes, and architectures. Specifically, we aim to investigate the following:

- **Correlation Analysis:** Examine how model size (number of parameters) and architecture type affect performance metrics across various evaluation tasks.
- **Performance Benchmarking:** Identify which models excel in specific tasks and understand the factors contributing to their success.

To achieve this, we will use Python for data analysis and visualization. Data manipulation and preprocessing will be handled using Pandas. For visualization, we will employ Vega Altair, a declarative statistical visualization library based on the Grammar of Graphics. These may include scatter plots to show correlations, heatmaps for performance comparisons, and bar charts to rank models based on specific criteria.

Our final deliverable will be a poster that includes both exploratory visualizations and communication graphics (infographics) that effectively summarize our key findings. The poster will also feature a reflection on our learning process, methodologies applied, and any challenges encountered during the project.

### Tools:

- Python, Pandas, Vega Altair

### Team:

- Katsiaryna Mlynchyk (mlynckat)
- Lars Schmid (schmila7)

### Date:

- 10. Oktober 2024, ZHAW Information Visualization
